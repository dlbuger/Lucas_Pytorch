{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as topti\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "from imdb_dataloader import IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Class for creating the neural network.\n",
    "class Network(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.in_dropout = tnn.Dropout(0.15)\n",
    "        self.lstm1 = tnn.LSTM(50, 300, 1,batch_first=True)\n",
    "        self.selu = tnn.SELU()\n",
    "        self.dropout = tnn.Dropout(0.5)\n",
    "        self.dense1 = tnn.Linear(300, 200)\n",
    "        self.norm1 = tnn.BatchNorm1d(200)\n",
    "        self.dense2 = tnn.Linear(200, 128)\n",
    "        self.dense3 = tnn.Linear(128,64)\n",
    "        self.norm2 = tnn.BatchNorm1d(64)\n",
    "        self.dense4 = tnn.Linear(64,1)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY FUNCTION SIGNATURE\n",
    "        Create the forward pass through the network.\n",
    "        \"\"\"\n",
    "        x = self.in_dropout(input)\n",
    "        o,(h_n,h_c) = self.lstm1(x)#[0][-1].view(-1,300)\n",
    "        x = self.selu(o[:,-1,:].view(-1,300))\n",
    "        x = self.dense1(x)\n",
    "        x = self.norm1(x)\n",
    "        x += (torch.randn(x.shape[1])*0.15).to(device)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.selu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense4(x)\n",
    "\n",
    "        \n",
    "        return (x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "\n",
    "    def pre(x):\n",
    "        \n",
    "        \"\"\"Called after tokenization 把句子拆成单词 \"\"\"\n",
    "        \"\"\"\n",
    "        GRAM too small ....\n",
    "        \"\"\"\n",
    "#         _arr = np.array([item for sublist in x for item in sublist]).reshape(1,-1)\n",
    "#         for i,dim in enumerate(_arr):\n",
    "#             _index = np.argwhere((dim == '/><br') == True).flatten()\n",
    "#             for j in _index:\n",
    "#                 _arr[i][j] = \"\"\n",
    "#                 try:\n",
    "#                     _arr[i][j-1] =_arr[i][j-1].replace(\"<br\")\n",
    "#                     _arr[i][j+1] =_arr[i][j+1].replace(\"/>\")\n",
    "#                 except IndexError:\n",
    "#                     print(\"outofbound\")\n",
    "#         #return _arr.flatten().tolist()\n",
    "        return x\n",
    "    def post(batch, vocab):\n",
    "    \n",
    "        return batch\n",
    "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunc():\n",
    "    \"\"\"\n",
    "    Define a loss function appropriate for the above networks that will\n",
    "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
    "    \"\"\"\n",
    "    return tnn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(_net,PATH='./model.pth',device=torch.device('cuda')):\n",
    "    _net.to(device)\n",
    "    torch.save(_net.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(_net):\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in testLoader:\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = torch.sigmoid(_net(inputs, length))\n",
    "            predicted = torch.round(outputs)\n",
    "            num_correct += torch.sum(labels == predicted).item()\n",
    "\n",
    "    accuracy = 100 * num_correct / len(dev)\n",
    "    print(f\"Classification accuracy: {accuracy}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "textField = PreProcessing.text_field\n",
    "labelField = data.Field(sequential=False)\n",
    "\n",
    "train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
    "\n",
    "textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
    "labelField.build_vocab(train, dev)\n",
    "\n",
    "trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
    "                                                     sort_key=lambda x: len(x.text), sort_within_batch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch:  1, Batch:   32, Loss: 0.749\n",
      "Epoch:  1, Batch:   64, Loss: 0.713\n",
      "Epoch:  1, Batch:   96, Loss: 0.690\n",
      "Epoch:  1, Batch:  128, Loss: 0.721\n",
      "Epoch:  1, Batch:  160, Loss: 0.715\n",
      "Epoch:  1, Batch:  192, Loss: 0.673\n",
      "Epoch:  1, Batch:  224, Loss: 0.630\n",
      "Epoch:  1, Batch:  256, Loss: 0.559\n",
      "Epoch:  1, Batch:  288, Loss: 0.560\n",
      "Epoch:  1, Batch:  320, Loss: 0.544\n",
      "Epoch:  1, Batch:  352, Loss: 0.502\n",
      "Epoch:  1, Batch:  384, Loss: 0.528\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 75.32010243277848\n",
      "No available model!\n",
      "Former Validation Acc\n",
      "Classification accuracy: 48.35147247119078\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch:  2, Batch:   32, Loss: 0.473\n",
      "Epoch:  2, Batch:   64, Loss: 0.513\n",
      "Epoch:  2, Batch:   96, Loss: 0.485\n",
      "Epoch:  2, Batch:  128, Loss: 0.491\n",
      "Epoch:  2, Batch:  160, Loss: 0.469\n",
      "Epoch:  2, Batch:  192, Loss: 0.477\n",
      "Epoch:  2, Batch:  224, Loss: 0.475\n",
      "Epoch:  2, Batch:  256, Loss: 0.474\n",
      "Epoch:  2, Batch:  288, Loss: 0.485\n",
      "Epoch:  2, Batch:  320, Loss: 0.463\n",
      "Epoch:  2, Batch:  352, Loss: 0.456\n",
      "Epoch:  2, Batch:  384, Loss: 0.477\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 77.01664532650449\n",
      "Former Validation Acc\n",
      "Classification accuracy: 75.30409731113957\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch:  3, Batch:   32, Loss: 0.452\n",
      "Epoch:  3, Batch:   64, Loss: 0.476\n",
      "Epoch:  3, Batch:   96, Loss: 0.487\n",
      "Epoch:  3, Batch:  128, Loss: 0.440\n",
      "Epoch:  3, Batch:  160, Loss: 0.450\n",
      "Epoch:  3, Batch:  192, Loss: 0.450\n",
      "Epoch:  3, Batch:  224, Loss: 0.477\n",
      "Epoch:  3, Batch:  256, Loss: 0.473\n",
      "Epoch:  3, Batch:  288, Loss: 0.471\n",
      "Epoch:  3, Batch:  320, Loss: 0.458\n",
      "Epoch:  3, Batch:  352, Loss: 0.456\n",
      "Epoch:  3, Batch:  384, Loss: 0.457\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 78.36107554417414\n",
      "Former Validation Acc\n",
      "Classification accuracy: 77.36875800256082\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch:  4, Batch:   32, Loss: 0.441\n",
      "Epoch:  4, Batch:   64, Loss: 0.442\n",
      "Epoch:  4, Batch:   96, Loss: 0.460\n",
      "Epoch:  4, Batch:  128, Loss: 0.455\n",
      "Epoch:  4, Batch:  160, Loss: 0.465\n",
      "Epoch:  4, Batch:  192, Loss: 0.457\n",
      "Epoch:  4, Batch:  224, Loss: 0.460\n",
      "Epoch:  4, Batch:  256, Loss: 0.451\n",
      "Epoch:  4, Batch:  288, Loss: 0.447\n",
      "Epoch:  4, Batch:  320, Loss: 0.440\n",
      "Epoch:  4, Batch:  352, Loss: 0.449\n",
      "Epoch:  4, Batch:  384, Loss: 0.443\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 78.45710627400769\n",
      "Former Validation Acc\n",
      "Classification accuracy: 78.69718309859155\n",
      "###########################################\n",
      "\n",
      "Epoch:  5, Batch:   32, Loss: 0.421\n",
      "Epoch:  5, Batch:   64, Loss: 0.438\n",
      "Epoch:  5, Batch:   96, Loss: 0.430\n",
      "Epoch:  5, Batch:  128, Loss: 0.445\n",
      "Epoch:  5, Batch:  160, Loss: 0.419\n",
      "Epoch:  5, Batch:  192, Loss: 0.466\n",
      "Epoch:  5, Batch:  224, Loss: 0.445\n",
      "Epoch:  5, Batch:  256, Loss: 0.454\n",
      "Epoch:  5, Batch:  288, Loss: 0.437\n",
      "Epoch:  5, Batch:  320, Loss: 0.431\n",
      "Epoch:  5, Batch:  352, Loss: 0.438\n",
      "Epoch:  5, Batch:  384, Loss: 0.438\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 78.87323943661971\n",
      "Former Validation Acc\n",
      "Classification accuracy: 78.63316261203585\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch:  6, Batch:   32, Loss: 0.417\n",
      "Epoch:  6, Batch:   64, Loss: 0.441\n",
      "Epoch:  6, Batch:   96, Loss: 0.428\n",
      "Epoch:  6, Batch:  128, Loss: 0.433\n",
      "Epoch:  6, Batch:  160, Loss: 0.441\n",
      "Epoch:  6, Batch:  192, Loss: 0.439\n",
      "Epoch:  6, Batch:  224, Loss: 0.418\n",
      "Epoch:  6, Batch:  256, Loss: 0.438\n",
      "Epoch:  6, Batch:  288, Loss: 0.425\n",
      "Epoch:  6, Batch:  320, Loss: 0.443\n",
      "Epoch:  6, Batch:  352, Loss: 0.440\n",
      "Epoch:  6, Batch:  384, Loss: 0.422\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 79.7055057618438\n",
      "Former Validation Acc\n",
      "Classification accuracy: 78.95326504481434\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch:  7, Batch:   32, Loss: 0.416\n",
      "Epoch:  7, Batch:   64, Loss: 0.439\n",
      "Epoch:  7, Batch:   96, Loss: 0.415\n",
      "Epoch:  7, Batch:  128, Loss: 0.428\n",
      "Epoch:  7, Batch:  160, Loss: 0.428\n",
      "Epoch:  7, Batch:  192, Loss: 0.408\n",
      "Epoch:  7, Batch:  224, Loss: 0.432\n",
      "Epoch:  7, Batch:  256, Loss: 0.436\n",
      "Epoch:  7, Batch:  288, Loss: 0.421\n",
      "Epoch:  7, Batch:  320, Loss: 0.423\n",
      "Epoch:  7, Batch:  352, Loss: 0.479\n",
      "Epoch:  7, Batch:  384, Loss: 0.442\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 79.46542893725993\n",
      "Former Validation Acc\n",
      "Classification accuracy: 79.60947503201024\n",
      "###########################################\n",
      "\n",
      "Epoch:  8, Batch:   32, Loss: 0.431\n",
      "Epoch:  8, Batch:   64, Loss: 0.428\n",
      "Epoch:  8, Batch:   96, Loss: 0.451\n",
      "Epoch:  8, Batch:  128, Loss: 0.428\n",
      "Epoch:  8, Batch:  160, Loss: 0.405\n",
      "Epoch:  8, Batch:  192, Loss: 0.407\n",
      "Epoch:  8, Batch:  224, Loss: 0.422\n",
      "Epoch:  8, Batch:  256, Loss: 0.432\n",
      "Epoch:  8, Batch:  288, Loss: 0.423\n",
      "Epoch:  8, Batch:  320, Loss: 0.423\n",
      "Epoch:  8, Batch:  352, Loss: 0.439\n",
      "Epoch:  8, Batch:  384, Loss: 0.412\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 80.26568501920615\n",
      "Former Validation Acc\n",
      "Classification accuracy: 79.7055057618438\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch:  9, Batch:   32, Loss: 0.412\n",
      "Epoch:  9, Batch:   64, Loss: 0.417\n",
      "Epoch:  9, Batch:   96, Loss: 0.427\n",
      "Epoch:  9, Batch:  128, Loss: 0.429\n",
      "Epoch:  9, Batch:  160, Loss: 0.433\n",
      "Epoch:  9, Batch:  192, Loss: 0.428\n",
      "Epoch:  9, Batch:  224, Loss: 0.426\n",
      "Epoch:  9, Batch:  256, Loss: 0.413\n",
      "Epoch:  9, Batch:  288, Loss: 0.418\n",
      "Epoch:  9, Batch:  320, Loss: 0.386\n",
      "Epoch:  9, Batch:  352, Loss: 0.406\n",
      "Epoch:  9, Batch:  384, Loss: 0.426\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 81.16197183098592\n",
      "Former Validation Acc\n",
      "Classification accuracy: 80.36171574903969\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 10, Batch:   32, Loss: 0.420\n",
      "Epoch: 10, Batch:   64, Loss: 0.428\n",
      "Epoch: 10, Batch:   96, Loss: 0.421\n",
      "Epoch: 10, Batch:  128, Loss: 0.407\n",
      "Epoch: 10, Batch:  160, Loss: 0.394\n",
      "Epoch: 10, Batch:  192, Loss: 0.404\n",
      "Epoch: 10, Batch:  224, Loss: 0.404\n",
      "Epoch: 10, Batch:  256, Loss: 0.411\n",
      "Epoch: 10, Batch:  288, Loss: 0.410\n",
      "Epoch: 10, Batch:  320, Loss: 0.397\n",
      "Epoch: 10, Batch:  352, Loss: 0.417\n",
      "Epoch: 10, Batch:  384, Loss: 0.401\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 80.74583866837388\n",
      "Former Validation Acc\n",
      "Classification accuracy: 80.39372599231754\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 11, Batch:   32, Loss: 0.406\n",
      "Epoch: 11, Batch:   64, Loss: 0.412\n",
      "Epoch: 11, Batch:   96, Loss: 0.393\n",
      "Epoch: 11, Batch:  128, Loss: 0.389\n",
      "Epoch: 11, Batch:  160, Loss: 0.395\n",
      "Epoch: 11, Batch:  192, Loss: 0.400\n",
      "Epoch: 11, Batch:  224, Loss: 0.405\n",
      "Epoch: 11, Batch:  256, Loss: 0.367\n",
      "Epoch: 11, Batch:  288, Loss: 0.405\n",
      "Epoch: 11, Batch:  320, Loss: 0.408\n",
      "Epoch: 11, Batch:  352, Loss: 0.381\n",
      "Epoch: 11, Batch:  384, Loss: 0.392\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 75.75224071702945\n",
      "Former Validation Acc\n",
      "Classification accuracy: 80.96991037131882\n",
      "###########################################\n",
      "\n",
      "Epoch: 12, Batch:   32, Loss: 0.470\n",
      "Epoch: 12, Batch:   64, Loss: 0.416\n",
      "Epoch: 12, Batch:   96, Loss: 0.394\n",
      "Epoch: 12, Batch:  128, Loss: 0.408\n",
      "Epoch: 12, Batch:  160, Loss: 0.385\n",
      "Epoch: 12, Batch:  192, Loss: 0.390\n",
      "Epoch: 12, Batch:  224, Loss: 0.381\n",
      "Epoch: 12, Batch:  256, Loss: 0.400\n",
      "Epoch: 12, Batch:  288, Loss: 0.416\n",
      "Epoch: 12, Batch:  320, Loss: 0.390\n",
      "Epoch: 12, Batch:  352, Loss: 0.406\n",
      "Epoch: 12, Batch:  384, Loss: 0.384\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 81.40204865556979\n",
      "Former Validation Acc\n",
      "Classification accuracy: 80.69782330345711\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 13, Batch:   32, Loss: 0.390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Batch:   64, Loss: 0.415\n",
      "Epoch: 13, Batch:   96, Loss: 0.382\n",
      "Epoch: 13, Batch:  128, Loss: 0.416\n",
      "Epoch: 13, Batch:  160, Loss: 0.393\n",
      "Epoch: 13, Batch:  192, Loss: 0.389\n",
      "Epoch: 13, Batch:  224, Loss: 0.384\n",
      "Epoch: 13, Batch:  256, Loss: 0.390\n",
      "Epoch: 13, Batch:  288, Loss: 0.382\n",
      "Epoch: 13, Batch:  320, Loss: 0.389\n",
      "Epoch: 13, Batch:  352, Loss: 0.382\n",
      "Epoch: 13, Batch:  384, Loss: 0.391\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 81.59411011523687\n",
      "Former Validation Acc\n",
      "Classification accuracy: 81.37003841229193\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 14, Batch:   32, Loss: 0.384\n",
      "Epoch: 14, Batch:   64, Loss: 0.375\n",
      "Epoch: 14, Batch:   96, Loss: 0.397\n",
      "Epoch: 14, Batch:  128, Loss: 0.375\n",
      "Epoch: 14, Batch:  160, Loss: 0.366\n",
      "Epoch: 14, Batch:  192, Loss: 0.371\n",
      "Epoch: 14, Batch:  224, Loss: 0.404\n",
      "Epoch: 14, Batch:  256, Loss: 0.361\n",
      "Epoch: 14, Batch:  288, Loss: 0.381\n",
      "Epoch: 14, Batch:  320, Loss: 0.372\n",
      "Epoch: 14, Batch:  352, Loss: 0.357\n",
      "Epoch: 14, Batch:  384, Loss: 0.388\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.19462227912932\n",
      "Former Validation Acc\n",
      "Classification accuracy: 80.93790012804098\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 15, Batch:   32, Loss: 0.380\n",
      "Epoch: 15, Batch:   64, Loss: 0.351\n",
      "Epoch: 15, Batch:   96, Loss: 0.362\n",
      "Epoch: 15, Batch:  128, Loss: 0.376\n",
      "Epoch: 15, Batch:  160, Loss: 0.350\n",
      "Epoch: 15, Batch:  192, Loss: 0.377\n",
      "Epoch: 15, Batch:  224, Loss: 0.360\n",
      "Epoch: 15, Batch:  256, Loss: 0.381\n",
      "Epoch: 15, Batch:  288, Loss: 0.379\n",
      "Epoch: 15, Batch:  320, Loss: 0.372\n",
      "Epoch: 15, Batch:  352, Loss: 0.359\n",
      "Epoch: 15, Batch:  384, Loss: 0.367\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 81.6741357234315\n",
      "Former Validation Acc\n",
      "Classification accuracy: 82.9865556978233\n",
      "###########################################\n",
      "\n",
      "Epoch: 16, Batch:   32, Loss: 0.361\n",
      "Epoch: 16, Batch:   64, Loss: 0.355\n",
      "Epoch: 16, Batch:   96, Loss: 0.357\n",
      "Epoch: 16, Batch:  128, Loss: 0.360\n",
      "Epoch: 16, Batch:  160, Loss: 0.372\n",
      "Epoch: 16, Batch:  192, Loss: 0.369\n",
      "Epoch: 16, Batch:  224, Loss: 0.337\n",
      "Epoch: 16, Batch:  256, Loss: 0.345\n",
      "Epoch: 16, Batch:  288, Loss: 0.357\n",
      "Epoch: 16, Batch:  320, Loss: 0.350\n",
      "Epoch: 16, Batch:  352, Loss: 0.360\n",
      "Epoch: 16, Batch:  384, Loss: 0.347\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.35467349551857\n",
      "Former Validation Acc\n",
      "Classification accuracy: 82.15428937259924\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 17, Batch:   32, Loss: 0.358\n",
      "Epoch: 17, Batch:   64, Loss: 0.348\n",
      "Epoch: 17, Batch:   96, Loss: 0.349\n",
      "Epoch: 17, Batch:  128, Loss: 0.349\n",
      "Epoch: 17, Batch:  160, Loss: 0.351\n",
      "Epoch: 17, Batch:  192, Loss: 0.366\n",
      "Epoch: 17, Batch:  224, Loss: 0.318\n",
      "Epoch: 17, Batch:  256, Loss: 0.346\n",
      "Epoch: 17, Batch:  288, Loss: 0.337\n",
      "Epoch: 17, Batch:  320, Loss: 0.344\n",
      "Epoch: 17, Batch:  352, Loss: 0.350\n",
      "Epoch: 17, Batch:  384, Loss: 0.347\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.53072983354673\n",
      "Former Validation Acc\n",
      "Classification accuracy: 83.59475032010243\n",
      "###########################################\n",
      "\n",
      "Epoch: 18, Batch:   32, Loss: 0.318\n",
      "Epoch: 18, Batch:   64, Loss: 0.354\n",
      "Epoch: 18, Batch:   96, Loss: 0.335\n",
      "Epoch: 18, Batch:  128, Loss: 0.320\n",
      "Epoch: 18, Batch:  160, Loss: 0.342\n",
      "Epoch: 18, Batch:  192, Loss: 0.352\n",
      "Epoch: 18, Batch:  224, Loss: 0.336\n",
      "Epoch: 18, Batch:  256, Loss: 0.366\n",
      "Epoch: 18, Batch:  288, Loss: 0.347\n",
      "Epoch: 18, Batch:  320, Loss: 0.324\n",
      "Epoch: 18, Batch:  352, Loss: 0.342\n",
      "Epoch: 18, Batch:  384, Loss: 0.338\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.21895006402049\n",
      "Former Validation Acc\n",
      "Classification accuracy: 83.46670934699104\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 19, Batch:   32, Loss: 0.326\n",
      "Epoch: 19, Batch:   64, Loss: 0.317\n",
      "Epoch: 19, Batch:   96, Loss: 0.340\n",
      "Epoch: 19, Batch:  128, Loss: 0.341\n",
      "Epoch: 19, Batch:  160, Loss: 0.326\n",
      "Epoch: 19, Batch:  192, Loss: 0.347\n",
      "Epoch: 19, Batch:  224, Loss: 0.318\n",
      "Epoch: 19, Batch:  256, Loss: 0.333\n",
      "Epoch: 19, Batch:  288, Loss: 0.348\n",
      "Epoch: 19, Batch:  320, Loss: 0.318\n",
      "Epoch: 19, Batch:  352, Loss: 0.316\n",
      "Epoch: 19, Batch:  384, Loss: 0.318\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.38668373879642\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.0909090909091\n",
      "###########################################\n",
      "\n",
      "Epoch: 20, Batch:   32, Loss: 0.328\n",
      "Epoch: 20, Batch:   64, Loss: 0.324\n",
      "Epoch: 20, Batch:   96, Loss: 0.346\n",
      "Epoch: 20, Batch:  128, Loss: 0.320\n",
      "Epoch: 20, Batch:  160, Loss: 0.312\n",
      "Epoch: 20, Batch:  192, Loss: 0.342\n",
      "Epoch: 20, Batch:  224, Loss: 0.309\n",
      "Epoch: 20, Batch:  256, Loss: 0.308\n",
      "Epoch: 20, Batch:  288, Loss: 0.318\n",
      "Epoch: 20, Batch:  320, Loss: 0.326\n",
      "Epoch: 20, Batch:  352, Loss: 0.314\n",
      "Epoch: 20, Batch:  384, Loss: 0.329\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.45070422535211\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.17093469910371\n",
      "###########################################\n",
      "\n",
      "Epoch: 21, Batch:   32, Loss: 0.305\n",
      "Epoch: 21, Batch:   64, Loss: 0.304\n",
      "Epoch: 21, Batch:   96, Loss: 0.314\n",
      "Epoch: 21, Batch:  128, Loss: 0.316\n",
      "Epoch: 21, Batch:  160, Loss: 0.310\n",
      "Epoch: 21, Batch:  192, Loss: 0.316\n",
      "Epoch: 21, Batch:  224, Loss: 0.305\n",
      "Epoch: 21, Batch:  256, Loss: 0.320\n",
      "Epoch: 21, Batch:  288, Loss: 0.305\n",
      "Epoch: 21, Batch:  320, Loss: 0.318\n",
      "Epoch: 21, Batch:  352, Loss: 0.333\n",
      "Epoch: 21, Batch:  384, Loss: 0.328\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.10691421254802\n",
      "Former Validation Acc\n",
      "Classification accuracy: 83.75480153649168\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 22, Batch:   32, Loss: 0.282\n",
      "Epoch: 22, Batch:   64, Loss: 0.348\n",
      "Epoch: 22, Batch:   96, Loss: 0.302\n",
      "Epoch: 22, Batch:  128, Loss: 0.301\n",
      "Epoch: 22, Batch:  160, Loss: 0.298\n",
      "Epoch: 22, Batch:  192, Loss: 0.317\n",
      "Epoch: 22, Batch:  224, Loss: 0.320\n",
      "Epoch: 22, Batch:  256, Loss: 0.301\n",
      "Epoch: 22, Batch:  288, Loss: 0.315\n",
      "Epoch: 22, Batch:  320, Loss: 0.292\n",
      "Epoch: 22, Batch:  352, Loss: 0.282\n",
      "Epoch: 22, Batch:  384, Loss: 0.288\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.71510883482715\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.34699103713189\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 23, Batch:   32, Loss: 0.275\n",
      "Epoch: 23, Batch:   64, Loss: 0.286\n",
      "Epoch: 23, Batch:   96, Loss: 0.282\n",
      "Epoch: 23, Batch:  128, Loss: 0.265\n",
      "Epoch: 23, Batch:  160, Loss: 0.297\n",
      "Epoch: 23, Batch:  192, Loss: 0.298\n",
      "Epoch: 23, Batch:  224, Loss: 0.302\n",
      "Epoch: 23, Batch:  256, Loss: 0.283\n",
      "Epoch: 23, Batch:  288, Loss: 0.290\n",
      "Epoch: 23, Batch:  320, Loss: 0.291\n",
      "Epoch: 23, Batch:  352, Loss: 0.296\n",
      "Epoch: 23, Batch:  384, Loss: 0.283\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.6190781049936\n",
      "Former Validation Acc\n",
      "Classification accuracy: 83.67477592829705\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 24, Batch:   32, Loss: 0.259\n",
      "Epoch: 24, Batch:   64, Loss: 0.271\n",
      "Epoch: 24, Batch:   96, Loss: 0.294\n",
      "Epoch: 24, Batch:  128, Loss: 0.287\n",
      "Epoch: 24, Batch:  160, Loss: 0.281\n",
      "Epoch: 24, Batch:  192, Loss: 0.268\n",
      "Epoch: 24, Batch:  224, Loss: 0.261\n",
      "Epoch: 24, Batch:  256, Loss: 0.286\n",
      "Epoch: 24, Batch:  288, Loss: 0.279\n",
      "Epoch: 24, Batch:  320, Loss: 0.291\n",
      "Epoch: 24, Batch:  352, Loss: 0.256\n",
      "Epoch: 24, Batch:  384, Loss: 0.276\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.07490396927017\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.84314980793854\n",
      "###########################################\n",
      "\n",
      "Epoch: 25, Batch:   32, Loss: 0.250\n",
      "Epoch: 25, Batch:   64, Loss: 0.262\n",
      "Epoch: 25, Batch:   96, Loss: 0.258\n",
      "Epoch: 25, Batch:  128, Loss: 0.278\n",
      "Epoch: 25, Batch:  160, Loss: 0.264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Batch:  192, Loss: 0.274\n",
      "Epoch: 25, Batch:  224, Loss: 0.256\n",
      "Epoch: 25, Batch:  256, Loss: 0.266\n",
      "Epoch: 25, Batch:  288, Loss: 0.256\n",
      "Epoch: 25, Batch:  320, Loss: 0.268\n",
      "Epoch: 25, Batch:  352, Loss: 0.257\n",
      "Epoch: 25, Batch:  384, Loss: 0.263\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.28297055057618\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.89116517285531\n",
      "###########################################\n",
      "\n",
      "Epoch: 26, Batch:   32, Loss: 0.252\n",
      "Epoch: 26, Batch:   64, Loss: 0.240\n",
      "Epoch: 26, Batch:   96, Loss: 0.248\n",
      "Epoch: 26, Batch:  128, Loss: 0.225\n",
      "Epoch: 26, Batch:  160, Loss: 0.238\n",
      "Epoch: 26, Batch:  192, Loss: 0.251\n",
      "Epoch: 26, Batch:  224, Loss: 0.253\n",
      "Epoch: 26, Batch:  256, Loss: 0.238\n",
      "Epoch: 26, Batch:  288, Loss: 0.260\n",
      "Epoch: 26, Batch:  320, Loss: 0.268\n",
      "Epoch: 26, Batch:  352, Loss: 0.271\n",
      "Epoch: 26, Batch:  384, Loss: 0.232\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.53905249679897\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.84314980793854\n",
      "###########################################\n",
      "\n",
      "Epoch: 27, Batch:   32, Loss: 0.239\n",
      "Epoch: 27, Batch:   64, Loss: 0.223\n",
      "Epoch: 27, Batch:   96, Loss: 0.231\n",
      "Epoch: 27, Batch:  128, Loss: 0.235\n",
      "Epoch: 27, Batch:  160, Loss: 0.236\n",
      "Epoch: 27, Batch:  192, Loss: 0.260\n",
      "Epoch: 27, Batch:  224, Loss: 0.234\n",
      "Epoch: 27, Batch:  256, Loss: 0.223\n",
      "Epoch: 27, Batch:  288, Loss: 0.245\n",
      "Epoch: 27, Batch:  320, Loss: 0.243\n",
      "Epoch: 27, Batch:  352, Loss: 0.254\n",
      "Epoch: 27, Batch:  384, Loss: 0.233\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.45902688860436\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.66709346991037\n",
      "###########################################\n",
      "\n",
      "Epoch: 28, Batch:   32, Loss: 0.221\n",
      "Epoch: 28, Batch:   64, Loss: 0.216\n",
      "Epoch: 28, Batch:   96, Loss: 0.204\n",
      "Epoch: 28, Batch:  128, Loss: 0.228\n",
      "Epoch: 28, Batch:  160, Loss: 0.229\n",
      "Epoch: 28, Batch:  192, Loss: 0.212\n",
      "Epoch: 28, Batch:  224, Loss: 0.224\n",
      "Epoch: 28, Batch:  256, Loss: 0.223\n",
      "Epoch: 28, Batch:  288, Loss: 0.226\n",
      "Epoch: 28, Batch:  320, Loss: 0.207\n",
      "Epoch: 28, Batch:  352, Loss: 0.226\n",
      "Epoch: 28, Batch:  384, Loss: 0.224\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.90717029449424\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.53905249679897\n",
      "\n",
      "Better Accuracy!!!\n",
      "Saved model\n",
      "###########################################\n",
      "\n",
      "Epoch: 29, Batch:   32, Loss: 0.182\n",
      "Epoch: 29, Batch:   64, Loss: 0.210\n",
      "Epoch: 29, Batch:   96, Loss: 0.187\n",
      "Epoch: 29, Batch:  128, Loss: 0.195\n",
      "Epoch: 29, Batch:  160, Loss: 0.206\n",
      "Epoch: 29, Batch:  192, Loss: 0.236\n",
      "Epoch: 29, Batch:  224, Loss: 0.210\n",
      "Epoch: 29, Batch:  256, Loss: 0.232\n",
      "Epoch: 29, Batch:  288, Loss: 0.194\n",
      "Epoch: 29, Batch:  320, Loss: 0.217\n",
      "Epoch: 29, Batch:  352, Loss: 0.213\n",
      "Epoch: 29, Batch:  384, Loss: 0.189\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.74711907810499\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.82714468629962\n",
      "###########################################\n",
      "\n",
      "Epoch: 30, Batch:   32, Loss: 0.176\n",
      "Epoch: 30, Batch:   64, Loss: 0.205\n",
      "Epoch: 30, Batch:   96, Loss: 0.159\n",
      "Epoch: 30, Batch:  128, Loss: 0.192\n",
      "Epoch: 30, Batch:  160, Loss: 0.208\n",
      "Epoch: 30, Batch:  192, Loss: 0.194\n",
      "Epoch: 30, Batch:  224, Loss: 0.197\n",
      "Epoch: 30, Batch:  256, Loss: 0.198\n",
      "Epoch: 30, Batch:  288, Loss: 0.205\n",
      "Epoch: 30, Batch:  320, Loss: 0.199\n",
      "Epoch: 30, Batch:  352, Loss: 0.204\n",
      "Epoch: 30, Batch:  384, Loss: 0.212\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.36299615877081\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.66709346991037\n",
      "###########################################\n",
      "\n",
      "Epoch: 31, Batch:   32, Loss: 0.178\n",
      "Epoch: 31, Batch:   64, Loss: 0.161\n",
      "Epoch: 31, Batch:   96, Loss: 0.185\n",
      "Epoch: 31, Batch:  128, Loss: 0.189\n",
      "Epoch: 31, Batch:  160, Loss: 0.178\n",
      "Epoch: 31, Batch:  192, Loss: 0.189\n",
      "Epoch: 31, Batch:  224, Loss: 0.186\n",
      "Epoch: 31, Batch:  256, Loss: 0.177\n",
      "Epoch: 31, Batch:  288, Loss: 0.192\n",
      "Epoch: 31, Batch:  320, Loss: 0.182\n",
      "Epoch: 31, Batch:  352, Loss: 0.223\n",
      "Epoch: 31, Batch:  384, Loss: 0.163\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.78681177976952\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.76312419974391\n",
      "###########################################\n",
      "\n",
      "Epoch: 32, Batch:   32, Loss: 0.158\n",
      "Epoch: 32, Batch:   64, Loss: 0.197\n",
      "Epoch: 32, Batch:   96, Loss: 0.161\n",
      "Epoch: 32, Batch:  128, Loss: 0.159\n",
      "Epoch: 32, Batch:  160, Loss: 0.163\n",
      "Epoch: 32, Batch:  192, Loss: 0.161\n",
      "Epoch: 32, Batch:  224, Loss: 0.189\n",
      "Epoch: 32, Batch:  256, Loss: 0.170\n",
      "Epoch: 32, Batch:  288, Loss: 0.174\n",
      "Epoch: 32, Batch:  320, Loss: 0.163\n",
      "Epoch: 32, Batch:  352, Loss: 0.177\n",
      "Epoch: 32, Batch:  384, Loss: 0.170\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.91485275288092\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.31498079385403\n",
      "###########################################\n",
      "\n",
      "Epoch: 33, Batch:   32, Loss: 0.133\n",
      "Epoch: 33, Batch:   64, Loss: 0.164\n",
      "Epoch: 33, Batch:   96, Loss: 0.134\n",
      "Epoch: 33, Batch:  128, Loss: 0.151\n",
      "Epoch: 33, Batch:  160, Loss: 0.157\n",
      "Epoch: 33, Batch:  192, Loss: 0.154\n",
      "Epoch: 33, Batch:  224, Loss: 0.150\n",
      "Epoch: 33, Batch:  256, Loss: 0.155\n",
      "Epoch: 33, Batch:  288, Loss: 0.142\n",
      "Epoch: 33, Batch:  320, Loss: 0.173\n",
      "Epoch: 33, Batch:  352, Loss: 0.173\n",
      "Epoch: 33, Batch:  384, Loss: 0.141\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 83.9628681177977\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.76312419974391\n",
      "###########################################\n",
      "\n",
      "Epoch: 34, Batch:   32, Loss: 0.124\n",
      "Epoch: 34, Batch:   64, Loss: 0.144\n",
      "Epoch: 34, Batch:   96, Loss: 0.137\n",
      "Epoch: 34, Batch:  128, Loss: 0.146\n",
      "Epoch: 34, Batch:  160, Loss: 0.151\n",
      "Epoch: 34, Batch:  192, Loss: 0.147\n",
      "Epoch: 34, Batch:  224, Loss: 0.150\n",
      "Epoch: 34, Batch:  256, Loss: 0.121\n",
      "Epoch: 34, Batch:  288, Loss: 0.125\n",
      "Epoch: 34, Batch:  320, Loss: 0.159\n",
      "Epoch: 34, Batch:  352, Loss: 0.162\n",
      "Epoch: 34, Batch:  384, Loss: 0.140\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 81.6101152368758\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.98719590268887\n",
      "###########################################\n",
      "\n",
      "Epoch: 35, Batch:   32, Loss: 0.164\n",
      "Epoch: 35, Batch:   64, Loss: 0.145\n",
      "Epoch: 35, Batch:   96, Loss: 0.155\n",
      "Epoch: 35, Batch:  128, Loss: 0.169\n",
      "Epoch: 35, Batch:  160, Loss: 0.146\n",
      "Epoch: 35, Batch:  192, Loss: 0.132\n",
      "Epoch: 35, Batch:  224, Loss: 0.155\n",
      "Epoch: 35, Batch:  256, Loss: 0.148\n",
      "Epoch: 35, Batch:  288, Loss: 0.125\n",
      "Epoch: 35, Batch:  320, Loss: 0.139\n",
      "Epoch: 35, Batch:  352, Loss: 0.140\n",
      "Epoch: 35, Batch:  384, Loss: 0.144\n",
      "\n",
      "###########################################\n",
      "Current Validation Acc\n",
      "Classification accuracy: 84.58706786171575\n",
      "Former Validation Acc\n",
      "Classification accuracy: 84.79513444302177\n",
      "###########################################\n",
      "\n",
      "Trainning Complete\n",
      "Best Result: \n",
      "Classification accuracy: 84.39500640204865\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Use a GPU if available, as it should be faster.\n",
    "    print(\"Using device: \" + str(device))\n",
    "\n",
    "    # Load the training dataset, and create a data loader to generate a batch.\n",
    "\n",
    "    net = Network().to(device)\n",
    "    criterion =lossFunc()\n",
    "    optimiser = topti.Adam(net.parameters(), lr=1e-3)  # Minimise the loss using the Adam algorithm.\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimiser,step_size=4, gamma=0.6)\n",
    "#     scheduler = torch.optim.lr_scheduler.ExponentialLR(optimiser, 0.9, last_epoch=-1)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser,10)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser,patience=5)\n",
    "    for epoch in range(35):\n",
    "        running_loss = 0\n",
    "        for i, batch in enumerate(trainLoader):\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            labels -= 1            \n",
    "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
    "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # Forward pass through the network.\n",
    "            output = torch.sigmoid(net(inputs, length))\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # Calculate gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Minimise the loss according to the gradient.\n",
    "            optimiser.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            if i % 32 == 31:\n",
    "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
    "                running_loss = 0\n",
    "        scheduler.step(running_loss)\n",
    "        \n",
    "    # Evaluation and save model\n",
    "        print()\n",
    "        print(\"###########################################\")\n",
    "        print(\"Current Validation Acc\")\n",
    "        new_acc = evaluate(net)\n",
    "        PATH = \"./model.pth\"\n",
    "        old_net = Network().to(device)\n",
    "        try:\n",
    "            old_net.load_state_dict(torch.load(PATH))\n",
    "        except OSError:\n",
    "            print(\"No available model!\")\n",
    "        print(\"Former Validation Acc\")\n",
    "        old_acc = evaluate(old_net)\n",
    "        if new_acc > old_acc:\n",
    "            save(net)\n",
    "            print()\n",
    "            print(\"Better Accuracy!!!\")\n",
    "            print(\"Saved model\")\n",
    "        print(\"###########################################\\n\")\n",
    "    print(\"Trainning Complete\")\n",
    "    \n",
    "    # Make the best acc model for cpu\n",
    "    net = Network().to(device)\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    print(\"Best Result: \")\n",
    "    acc = evaluate(net)\n",
    "    save(net,device=torch.device('cpu'))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
