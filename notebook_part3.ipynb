{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as topti\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "from imdb_dataloader import IMDB\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "textField = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "labelField = data.Field(sequential=False)\n",
    "\n",
    "from imdb_dataloader import IMDB\n",
    "train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
    "\n",
    "textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
    "labelField.build_vocab(train, dev)\n",
    "\n",
    "trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
    "                                                     sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = tnn.LSTM(50, 200, batch_first=True)\n",
    "\n",
    "conv = tnn.Conv1d(1, 24, 5, padding=5)\n",
    "pool = tnn.MaxPool1d(4)\n",
    "dense = tnn.Linear(24*51,400)\n",
    "lstm2 = tnn.LSTM(400, 256,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "for i, batch in enumerate(trainLoader, 0):\n",
    "                inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "                labels -= 1\n",
    "                o  = lstm(inputs)[1][0].permute(1,0,2)   \n",
    "                o  = conv(o)\n",
    "                o = pool(o)\n",
    "                o = dense(o.view(-1,24*51))\n",
    "                o = lstm2(o.view(-1,1,400))[1][0]\n",
    "                shapes.append(o.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "o  = lstm(inputs)[1][0].permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 24, 49])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 40, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256]),\n",
       " torch.Size([1, 64, 256])]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for creating the neural network.\n",
    "class Network(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.criterion = None     \n",
    "\n",
    "        self.lstm1 = tnn.LSTM(50, 200, batch_first=True)\n",
    "        self.conv1 = tnn.Conv1d(1, 24, 5, padding=5)\n",
    "        self.relu = tnn.ReLU()\n",
    "        self.maxpool = tnn.MaxPool1d(4)\n",
    "        # TODO -> 先Dropout还是先normalize ？\n",
    "        self.dropout = tnn.Dropout()\n",
    "        \n",
    "        self.dense1 = tnn.Linear(24*51, 400)\n",
    "        self.norm1 = tnn.BatchNorm1d(400)\n",
    "        \n",
    "\n",
    "        # Relu\n",
    "        self.dense2 = tnn.Linear(400, 256)\n",
    "        # dropout\n",
    "        self.dense3 = tnn.Linear(256, 64)\n",
    "        # Relu\n",
    "        self.dense4 = tnn.Linear(64, 1)\n",
    "        \n",
    "        self.to(self.device)\n",
    "        print(self.device)    \n",
    "        \n",
    "        \n",
    "    def compile(self, optimizer, criterion):\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit(self, trainset, EPOCHS):\n",
    "        for epoch in range(EPOCHS):\n",
    "            running_loss = 0.0\n",
    "            for i, batch in enumerate(trainset, 0):\n",
    "                inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "                labels -= 1\n",
    "                self.optimizer.zero_grad()\n",
    "                #tnn.functional.batch_norm(inputs,0.5,0.5,training=True)\n",
    "                \n",
    "                predict = self(inputs.to(self.device),length)\n",
    "                loss = criterion(predict.view(-1), labels.to(self.device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                if i % 32 == 31:\n",
    "                    print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % \n",
    "                              (epoch + 1, i + 1, running_loss / 32))\n",
    "                    running_loss = 0\n",
    "        print('trainning completed!')\n",
    "                \n",
    "        \n",
    "    def conv(self, features):\n",
    "        x = self.lstm1(features)[1][0].permute(1,0,2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "    \n",
    "        return x\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        x = self.conv(input)\n",
    "        x = self.dense1(x.view(-1,24*51))\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        return tnn.functional.logsigmoid(x, dim=1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Network(\n",
      "  (lstm1): LSTM(50, 200, batch_first=True)\n",
      "  (conv1): Conv1d(1, 24, kernel_size=(5,), stride=(1,), padding=(5,))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (dense1): Linear(in_features=1224, out_features=400, bias=True)\n",
      "  (norm1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dense2): Linear(in_features=400, out_features=256, bias=True)\n",
      "  (dense3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (dense4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = tnn.MSELoss()\n",
    "net.compile(optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reco/Applications/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Batch:   32, Loss: 0.498\n",
      "Epoch:  1, Batch:   64, Loss: 0.511\n",
      "Epoch:  1, Batch:   96, Loss: 0.488\n",
      "Epoch:  1, Batch:  128, Loss: 0.494\n",
      "Epoch:  1, Batch:  160, Loss: 0.479\n",
      "Epoch:  1, Batch:  192, Loss: 0.506\n",
      "Epoch:  1, Batch:  224, Loss: 0.494\n",
      "Epoch:  1, Batch:  256, Loss: 0.518\n",
      "Epoch:  1, Batch:  288, Loss: 0.490\n",
      "Epoch:  1, Batch:  320, Loss: 0.502\n",
      "Epoch:  1, Batch:  352, Loss: 0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reco/Applications/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Batch:  384, Loss: 0.529\n",
      "Epoch:  2, Batch:   32, Loss: 0.490\n",
      "Epoch:  2, Batch:   64, Loss: 0.511\n",
      "Epoch:  2, Batch:   96, Loss: 0.519\n",
      "Epoch:  2, Batch:  128, Loss: 0.480\n",
      "Epoch:  2, Batch:  160, Loss: 0.510\n",
      "Epoch:  2, Batch:  192, Loss: 0.517\n",
      "Epoch:  2, Batch:  224, Loss: 0.486\n",
      "Epoch:  2, Batch:  256, Loss: 0.510\n",
      "Epoch:  2, Batch:  288, Loss: 0.485\n",
      "Epoch:  2, Batch:  320, Loss: 0.480\n",
      "Epoch:  2, Batch:  352, Loss: 0.515\n",
      "Epoch:  2, Batch:  384, Loss: 0.498\n",
      "Epoch:  3, Batch:   32, Loss: 0.511\n",
      "Epoch:  3, Batch:   64, Loss: 0.497\n",
      "Epoch:  3, Batch:   96, Loss: 0.503\n",
      "Epoch:  3, Batch:  128, Loss: 0.491\n",
      "Epoch:  3, Batch:  160, Loss: 0.504\n",
      "Epoch:  3, Batch:  192, Loss: 0.500\n",
      "Epoch:  3, Batch:  224, Loss: 0.506\n",
      "Epoch:  3, Batch:  256, Loss: 0.488\n",
      "Epoch:  3, Batch:  288, Loss: 0.502\n",
      "Epoch:  3, Batch:  320, Loss: 0.490\n",
      "Epoch:  3, Batch:  352, Loss: 0.481\n",
      "Epoch:  3, Batch:  384, Loss: 0.515\n",
      "trainning completed!\n"
     ]
    }
   ],
   "source": [
    "net.fit(trainLoader,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def pre(x):\n",
    "        \"\"\"Called after tokenization\"\"\"\n",
    "        return x\n",
    "\n",
    "    def post(batch, vocab):\n",
    "        \"\"\"Called after numericalization but prior to vectorization\"\"\"\n",
    "        return batch, vocab\n",
    "\n",
    "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunc():\n",
    "    \"\"\"\n",
    "    Define a loss function appropriate for the above networks that will\n",
    "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Use a GPU if available, as it should be faster.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \" + str(device))\n",
    "\n",
    "    # Load the training dataset, and create a data loader to generate a batch.\n",
    "    textField = PreProcessing.text_field\n",
    "    labelField = data.Field(sequential=False)\n",
    "\n",
    "    train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
    "\n",
    "    textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
    "    labelField.build_vocab(train, dev)\n",
    "\n",
    "    trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
    "                                                         sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "\n",
    "    net = Network().to(device)\n",
    "    criterion =lossFunc()\n",
    "    optimiser = topti.Adam(net.parameters(), lr=0.001)  # Minimise the loss using the Adam algorithm.\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(trainLoader):\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
    "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Forward pass through the network.\n",
    "            output = net(inputs, length)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # Calculate gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Minimise the loss according to the gradient.\n",
    "            optimiser.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 32 == 31:\n",
    "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
    "                running_loss = 0\n",
    "\n",
    "    num_correct = 0\n",
    "\n",
    "    # Save mode\n",
    "    torch.save(net.state_dict(), \"./model.pth\")\n",
    "    print(\"Saved model\")\n",
    "\n",
    "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
    "    # computations and reduce memory usage.\n",
    "    with torch.no_grad():\n",
    "        for batch in testLoader:\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = torch.sigmoid(net(inputs, length))\n",
    "            predicted = torch.round(outputs)\n",
    "\n",
    "            num_correct += torch.sum(labels == predicted).item()\n",
    "\n",
    "    accuracy = 100 * num_correct / len(dev)\n",
    "\n",
    "    print(f\"Classification accuracy: {accuracy}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
