{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(rnn, self).__init__()\n",
    "\n",
    "        self.ih = torch.nn.Linear(64, 128)\n",
    "        self.hh = torch.nn.Linear(128, 128)\n",
    "\n",
    "    def rnnCell(self, input, hidden):\n",
    "        \"\"\"\n",
    "        TODO: Using only the above defined linear layers and a tanh\n",
    "              activation, create an Elman RNN cell.  You do not need\n",
    "              to include an output layer.  The network should take\n",
    "              some input (inputDim = 64) and the current hidden state\n",
    "              (hiddenDim = 128), and return the new hidden state.\n",
    "        \"\"\"\n",
    "        return torch.tanh(self.ih(input) + self.hh(hidden))\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = torch.zeros(128)\n",
    "        \"\"\"\n",
    "        TODO: Using self.rnnCell, create a model that takes as input\n",
    "              a sequence of size [seqLength, batchSize, inputDim]\n",
    "              and passes each input through the rnn sequentially,\n",
    "              updating the (initally zero) hidden state.\n",
    "              Return the final hidden state after the\n",
    "              last input in the sequence has been processed.\n",
    "        \"\"\"\n",
    "        for seq in input:\n",
    "            hidden = self.rnnCell(seq, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnnSimplified(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(rnnSimplified, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO: Define self.net using a single PyTorch module such that\n",
    "              the network defined by this class is equivalent to the\n",
    "              one defined in class \"rnn\".\n",
    "        \"\"\"\n",
    "        self.net = torch.nn.RNN(64,128)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = torch.zeros(1,1,128)\n",
    "        _, hidden = self.net(input, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(input, hiddenSize):\n",
    "    \"\"\"\n",
    "    TODO: Let variable lstm be an instance of torch.nn.LSTM.\n",
    "          Variable input is of size [batchSize, seqLength, inputDim]\n",
    "    \"\"\"\n",
    "    lstm = torch.nn.LSTM(input.shape[2],hiddenSize, batch_first=True)\n",
    "    return lstm(input)\n",
    "\n",
    "def conv(input, weight):\n",
    "    \"\"\"\n",
    "    TODO: Return the convolution of input and weight tensors,\n",
    "          where input contains sequential data.\n",
    "          The convolution should be along the sequence axis.\n",
    "          input is of size [batchSize, inputDim, seqLength]\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.conv1d(input.shape[0], weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试神经网络是否正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(20, 64)\n",
    "y = torch.ones(128)\n",
    "\n",
    "net = rnn()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-4)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.957676483144155e-07\n",
      "8.174656773718425e-08\n",
      "3.5554505894985766e-08\n",
      "1.5948500353690775e-08\n",
      "7.314123751100965e-09\n",
      "3.4046740149396015e-09\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "for e in range(30):\n",
    "    _loss = 0\n",
    "    for i,data in enumerate(x):\n",
    "        output = net(x)\n",
    "        loss = loss_func(y, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _loss += loss.item()\n",
    "    if e % 5 == 0:\n",
    "        print(_loss)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8167, 1.0000, 0.9774, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9829, 0.9990, 0.9771, 0.6492, 0.9615, 1.0000,\n",
      "        0.9202, 1.0000, 1.0000, 0.9698, 1.0000, 1.0000, 1.0000, 0.8850, 1.0000,\n",
      "        1.0000, 0.3096, 0.9613, 0.9999, 0.9942, 0.9986, 0.9999, 1.0000, 0.9991,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.9991, 0.9943, 1.0000, 1.0000, 0.9948,\n",
      "        1.0000, 0.9206, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 0.9998, 0.9991,\n",
      "        1.0000, 0.9621, 1.0000, 1.0000, 1.0000, 0.9803, 0.9968, 1.0000, 0.9582,\n",
      "        0.9990, 1.0000, 1.0000, 1.0000, 1.0000, 0.9900, 0.9996, 1.0000, 0.9901,\n",
      "        0.8392, 0.9924, 1.0000, 0.8318, 1.0000, 0.9999, 0.9900, 0.9960, 0.9761,\n",
      "        1.0000, 0.7626, 0.9985, 0.9999, 1.0000, 1.0000, 0.5686, 0.9971, 0.9999,\n",
      "        0.9999, 0.9999, 1.0000, 0.9999, 0.9269, 1.0000, 0.9733, 1.0000, 1.0000,\n",
      "        1.0000, 0.9785, 1.0000, 1.0000, 1.0000, 0.6117, 0.9876, 0.7154, 0.9999,\n",
      "        1.0000, 0.9794, 1.0000, 1.0000, 0.8731, 0.7549, 0.9929, 1.0000, 0.9977,\n",
      "        0.9981, 0.4446, 1.0000, 1.0000, 1.0000, 0.8300, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000])\n",
      "tensor(4.8004)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = net(torch.rand(1,64))\n",
    "    cost = 1 - pred\n",
    "    print(pred)\n",
    "    print(cost.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnnSimplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(20, 64)\n",
    "y = torch.ones(128)\n",
    "\n",
    "net = rnnSimplified()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### net.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.63486647605896\n",
      "6.97745606303215\n",
      "1.6615901067852974\n",
      "0.35867406334728\n",
      "0.0789528206223622\n",
      "0.01815788417297881\n",
      "tensor([[[0.9935, 0.9830, 0.9854, 0.9886, 0.9867, 0.9779, 0.9927, 0.9928,\n",
      "          0.9802, 0.9732, 0.9849, 0.9816, 0.9810, 0.9562, 0.9862, 0.9938,\n",
      "          0.9927, 0.9885, 0.9873, 0.9823, 0.9755, 0.9814, 0.9710, 0.9853,\n",
      "          0.9884, 0.9872, 0.9877, 0.9827, 0.9914, 0.9849, 0.9874, 0.9745,\n",
      "          0.9846, 0.9796, 0.9936, 0.9877, 0.9856, 0.9924, 0.9896, 0.9811,\n",
      "          0.9869, 0.9881, 0.9891, 0.9795, 0.9865, 0.9880, 0.9877, 0.9878,\n",
      "          0.9934, 0.9753, 0.9805, 0.9907, 0.9902, 0.9886, 0.9806, 0.9850,\n",
      "          0.9916, 0.9872, 0.9936, 0.9892, 0.9412, 0.9907, 0.9742, 0.9780,\n",
      "          0.9880, 0.9807, 0.9846, 0.9878, 0.9918, 0.9831, 0.9874, 0.9845,\n",
      "          0.9870, 0.9775, 0.9842, 0.9784, 0.9908, 0.9868, 0.9884, 0.9799,\n",
      "          0.9878, 0.9903, 0.9853, 0.9833, 0.9875, 0.9820, 0.9746, 0.9665,\n",
      "          0.9846, 0.9776, 0.9915, 0.9906, 0.9794, 0.9600, 0.9821, 0.9850,\n",
      "          0.9929, 0.9899, 0.9905, 0.9713, 0.9905, 0.9904, 0.9859, 0.9846,\n",
      "          0.9697, 0.9766, 0.9905, 0.9883, 0.9917, 0.9861, 0.9715, 0.9865,\n",
      "          0.9873, 0.9847, 0.9837, 0.9855, 0.9930, 0.9858, 0.9933, 0.9823,\n",
      "          0.9872, 0.9856, 0.9843, 0.9827, 0.9833, 0.9893, 0.9729, 0.9885]]],\n",
      "       grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    " for e in range(30):\n",
    "    _loss = 0\n",
    "    for i, data in enumerate(x):\n",
    "        output = net(data.view(1,1,-1))\n",
    "        loss = loss_func(y, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _loss += loss.item()\n",
    "    if e % 5 == 0:\n",
    "        print(_loss)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9906, 0.9893, 0.9841, 0.9903, 0.9893, 0.9857, 0.9954, 0.9908,\n",
      "          0.9849, 0.9787, 0.9916, 0.9840, 0.9914, 0.9793, 0.9902, 0.9937,\n",
      "          0.9924, 0.9848, 0.9926, 0.9863, 0.9796, 0.9826, 0.9860, 0.9878,\n",
      "          0.9909, 0.9854, 0.9882, 0.9871, 0.9933, 0.9927, 0.9896, 0.9920,\n",
      "          0.9914, 0.9855, 0.9953, 0.9904, 0.9891, 0.9909, 0.9917, 0.9832,\n",
      "          0.9920, 0.9901, 0.9882, 0.9806, 0.9891, 0.9922, 0.9786, 0.9865,\n",
      "          0.9926, 0.9796, 0.9904, 0.9939, 0.9960, 0.9911, 0.9877, 0.9930,\n",
      "          0.9934, 0.9940, 0.9959, 0.9928, 0.9795, 0.9791, 0.9817, 0.9856,\n",
      "          0.9925, 0.9867, 0.9843, 0.9915, 0.9911, 0.9929, 0.9881, 0.9835,\n",
      "          0.9919, 0.9924, 0.9932, 0.9871, 0.9870, 0.9904, 0.9876, 0.9845,\n",
      "          0.9900, 0.9909, 0.9874, 0.9867, 0.9891, 0.9867, 0.9815, 0.9799,\n",
      "          0.9902, 0.9891, 0.9951, 0.9917, 0.9857, 0.9839, 0.9835, 0.9915,\n",
      "          0.9904, 0.9916, 0.9909, 0.9740, 0.9921, 0.9908, 0.9943, 0.9881,\n",
      "          0.9867, 0.9857, 0.9932, 0.9884, 0.9924, 0.9905, 0.9814, 0.9892,\n",
      "          0.9916, 0.9893, 0.9919, 0.9921, 0.9946, 0.9874, 0.9931, 0.9769,\n",
      "          0.9932, 0.9876, 0.9808, 0.9805, 0.9838, 0.9924, 0.9829, 0.9867]]])\n",
      "tensor(1.4864)\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "with torch.no_grad():\n",
    "    pred = net(torch.rand(1,1,64))\n",
    "    cost = 1 - pred\n",
    "    print(pred)\n",
    "    print(cost.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10, 20, 64)\n",
    "_lstm = lstm(x, 64, )\n",
    "print(_lstm[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
