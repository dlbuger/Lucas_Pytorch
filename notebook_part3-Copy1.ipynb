{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as topti\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "from imdb_dataloader import IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for creating the neural network.\n",
    "class Network(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(50, 50, 8, padding=5,)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.pool1 = torch.nn.MaxPool1d(4)\n",
    "        self.norm = torch.nn.BatchNorm2d(50)\n",
    "        self.conv2 = torch.nn.Conv1d(50, 50, 8, padding=5)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        # ReLu\n",
    "        self.pool2 = torch.nn.MaxPool1d(4) # shape (batch_size, channel)\n",
    "        self.conv3 = torch.nn.Conv1d(50, 50, 8, padding=5)\n",
    "        # ReLu\n",
    "        self.global_pool = torch.nn.functional.max_pool1d\n",
    "        self.dense = torch.nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, input, length):\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY FUNCTION SIGNATURE\n",
    "        Create the forward pass through the network.\n",
    "        \"\"\"\n",
    "        x = self.conv1(input.permute(0,2,1))\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.global_pool(x,kernel_size=x.shape[2]).view(-1,50)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return (x)\n",
    "\n",
    "        # TODO Weight Normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 83.01856594110116\n",
    "# 83.67477592829705\n",
    "# 83.91485275288092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "\n",
    "    def pre(x):\n",
    "        \n",
    "        \"\"\"Called after tokenization 把句子拆成单词 \"\"\"\n",
    "        \"\"\"\n",
    "        GRAM too small ....\n",
    "        \"\"\"\n",
    "#         _arr = np.array([item for sublist in x for item in sublist]).reshape(1,-1)\n",
    "#         for i,dim in enumerate(_arr):\n",
    "#             _index = np.argwhere((dim == '/><br') == True).flatten()\n",
    "#             for j in _index:\n",
    "#                 _arr[i][j] = \"\"|\n",
    "#                 try:\n",
    "#                     _arr[i][j-1] =_arr[i][j-1].replace(\"<br\")\n",
    "#                     _arr[i][j+1] =_arr[i][j+1].replace(\"/>\")\n",
    "#                 except IndexError:\n",
    "#                     print(\"outofbound\")\n",
    "#         #return _arr.flatten().tolist()\n",
    "        return x\n",
    "    def post(batch, vocab):\n",
    "        \n",
    "        \n",
    "        #return torch.nn.functional.normalize(torch.FloatTensor(batch))\n",
    "        return batch\n",
    "\n",
    "    text_field = data.Field(lower=True, include_lengths=True, batch_first=True, preprocessing=pre, postprocessing=post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunc():\n",
    "    \"\"\"\n",
    "    Define a loss function appropriate for the above networks that will\n",
    "    add a sigmoid to the output and calculate the binary cross-entropy.\n",
    "    \"\"\"\n",
    "    return tnn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Epoch:  1, Batch:   32, Loss: 0.695\n",
      "Epoch:  1, Batch:   64, Loss: 0.684\n",
      "Epoch:  1, Batch:   96, Loss: 0.654\n",
      "Epoch:  1, Batch:  128, Loss: 0.591\n",
      "Epoch:  1, Batch:  160, Loss: 0.543\n",
      "Epoch:  1, Batch:  192, Loss: 0.518\n",
      "Epoch:  1, Batch:  224, Loss: 0.513\n",
      "Epoch:  1, Batch:  256, Loss: 0.479\n",
      "Epoch:  1, Batch:  288, Loss: 0.518\n",
      "Epoch:  1, Batch:  320, Loss: 0.487\n",
      "Epoch:  1, Batch:  352, Loss: 0.481\n",
      "Epoch:  1, Batch:  384, Loss: 0.489\n",
      "Epoch:  2, Batch:   32, Loss: 0.423\n",
      "Epoch:  2, Batch:   64, Loss: 0.443\n",
      "Epoch:  2, Batch:   96, Loss: 0.439\n",
      "Epoch:  2, Batch:  128, Loss: 0.443\n",
      "Epoch:  2, Batch:  160, Loss: 0.429\n",
      "Epoch:  2, Batch:  192, Loss: 0.437\n",
      "Epoch:  2, Batch:  224, Loss: 0.406\n",
      "Epoch:  2, Batch:  256, Loss: 0.398\n",
      "Epoch:  2, Batch:  288, Loss: 0.406\n",
      "Epoch:  2, Batch:  320, Loss: 0.425\n",
      "Epoch:  2, Batch:  352, Loss: 0.447\n",
      "Epoch:  2, Batch:  384, Loss: 0.437\n",
      "Epoch:  3, Batch:   32, Loss: 0.387\n",
      "Epoch:  3, Batch:   64, Loss: 0.380\n",
      "Epoch:  3, Batch:   96, Loss: 0.362\n",
      "Epoch:  3, Batch:  128, Loss: 0.380\n",
      "Epoch:  3, Batch:  160, Loss: 0.397\n",
      "Epoch:  3, Batch:  192, Loss: 0.385\n",
      "Epoch:  3, Batch:  224, Loss: 0.367\n",
      "Epoch:  3, Batch:  256, Loss: 0.397\n",
      "Epoch:  3, Batch:  288, Loss: 0.373\n",
      "Epoch:  3, Batch:  320, Loss: 0.376\n",
      "Epoch:  3, Batch:  352, Loss: 0.364\n",
      "Epoch:  3, Batch:  384, Loss: 0.362\n",
      "Epoch:  4, Batch:   32, Loss: 0.336\n",
      "Epoch:  4, Batch:   64, Loss: 0.338\n",
      "Epoch:  4, Batch:   96, Loss: 0.343\n",
      "Epoch:  4, Batch:  128, Loss: 0.345\n",
      "Epoch:  4, Batch:  160, Loss: 0.409\n",
      "Epoch:  4, Batch:  192, Loss: 0.363\n",
      "Epoch:  4, Batch:  224, Loss: 0.353\n",
      "Epoch:  4, Batch:  256, Loss: 0.334\n",
      "Epoch:  4, Batch:  288, Loss: 0.360\n",
      "Epoch:  4, Batch:  320, Loss: 0.332\n",
      "Epoch:  4, Batch:  352, Loss: 0.327\n",
      "Epoch:  4, Batch:  384, Loss: 0.333\n",
      "Epoch:  5, Batch:   32, Loss: 0.295\n",
      "Epoch:  5, Batch:   64, Loss: 0.331\n",
      "Epoch:  5, Batch:   96, Loss: 0.299\n",
      "Epoch:  5, Batch:  128, Loss: 0.314\n",
      "Epoch:  5, Batch:  160, Loss: 0.304\n",
      "Epoch:  5, Batch:  192, Loss: 0.289\n",
      "Epoch:  5, Batch:  224, Loss: 0.301\n",
      "Epoch:  5, Batch:  256, Loss: 0.304\n",
      "Epoch:  5, Batch:  288, Loss: 0.327\n",
      "Epoch:  5, Batch:  320, Loss: 0.321\n",
      "Epoch:  5, Batch:  352, Loss: 0.296\n",
      "Epoch:  5, Batch:  384, Loss: 0.279\n",
      "Epoch:  6, Batch:   32, Loss: 0.286\n",
      "Epoch:  6, Batch:   64, Loss: 0.303\n",
      "Epoch:  6, Batch:   96, Loss: 0.275\n",
      "Epoch:  6, Batch:  128, Loss: 0.283\n",
      "Epoch:  6, Batch:  160, Loss: 0.273\n",
      "Epoch:  6, Batch:  192, Loss: 0.268\n",
      "Epoch:  6, Batch:  224, Loss: 0.266\n",
      "Epoch:  6, Batch:  256, Loss: 0.281\n",
      "Epoch:  6, Batch:  288, Loss: 0.312\n",
      "Epoch:  6, Batch:  320, Loss: 0.315\n",
      "Epoch:  6, Batch:  352, Loss: 0.277\n",
      "Epoch:  6, Batch:  384, Loss: 0.272\n",
      "Epoch:  7, Batch:   32, Loss: 0.232\n",
      "Epoch:  7, Batch:   64, Loss: 0.249\n",
      "Epoch:  7, Batch:   96, Loss: 0.247\n",
      "Epoch:  7, Batch:  128, Loss: 0.233\n",
      "Epoch:  7, Batch:  160, Loss: 0.255\n",
      "Epoch:  7, Batch:  192, Loss: 0.240\n",
      "Epoch:  7, Batch:  224, Loss: 0.226\n",
      "Epoch:  7, Batch:  256, Loss: 0.270\n",
      "Epoch:  7, Batch:  288, Loss: 0.260\n",
      "Epoch:  7, Batch:  320, Loss: 0.225\n",
      "Epoch:  7, Batch:  352, Loss: 0.240\n",
      "Epoch:  7, Batch:  384, Loss: 0.228\n",
      "Epoch:  8, Batch:   32, Loss: 0.215\n",
      "Epoch:  8, Batch:   64, Loss: 0.223\n",
      "Epoch:  8, Batch:   96, Loss: 0.202\n",
      "Epoch:  8, Batch:  128, Loss: 0.230\n",
      "Epoch:  8, Batch:  160, Loss: 0.230\n",
      "Epoch:  8, Batch:  192, Loss: 0.234\n",
      "Epoch:  8, Batch:  224, Loss: 0.221\n",
      "Epoch:  8, Batch:  256, Loss: 0.212\n",
      "Epoch:  8, Batch:  288, Loss: 0.216\n",
      "Epoch:  8, Batch:  320, Loss: 0.222\n",
      "Epoch:  8, Batch:  352, Loss: 0.225\n",
      "Epoch:  8, Batch:  384, Loss: 0.235\n",
      "Epoch:  9, Batch:   32, Loss: 0.202\n",
      "Epoch:  9, Batch:   64, Loss: 0.200\n",
      "Epoch:  9, Batch:   96, Loss: 0.179\n",
      "Epoch:  9, Batch:  128, Loss: 0.194\n",
      "Epoch:  9, Batch:  160, Loss: 0.197\n",
      "Epoch:  9, Batch:  192, Loss: 0.198\n",
      "Epoch:  9, Batch:  224, Loss: 0.195\n",
      "Epoch:  9, Batch:  256, Loss: 0.221\n",
      "Epoch:  9, Batch:  288, Loss: 0.188\n",
      "Epoch:  9, Batch:  320, Loss: 0.210\n",
      "Epoch:  9, Batch:  352, Loss: 0.189\n",
      "Epoch:  9, Batch:  384, Loss: 0.210\n",
      "Epoch: 10, Batch:   32, Loss: 0.195\n",
      "Epoch: 10, Batch:   64, Loss: 0.165\n",
      "Epoch: 10, Batch:   96, Loss: 0.180\n",
      "Epoch: 10, Batch:  128, Loss: 0.165\n",
      "Epoch: 10, Batch:  160, Loss: 0.189\n",
      "Epoch: 10, Batch:  192, Loss: 0.183\n",
      "Epoch: 10, Batch:  224, Loss: 0.173\n",
      "Epoch: 10, Batch:  256, Loss: 0.205\n",
      "Epoch: 10, Batch:  288, Loss: 0.176\n",
      "Epoch: 10, Batch:  320, Loss: 0.191\n",
      "Epoch: 10, Batch:  352, Loss: 0.186\n",
      "Epoch: 10, Batch:  384, Loss: 0.191\n",
      "Classification accuracy: 82.93854033290653\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Use a GPU if available, as it should be faster.\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \" + str(device))\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load the training dataset, and create a data loader to generate a batch.\n",
    "    textField = PreProcessing.text_field\n",
    "    labelField = data.Field(sequential=False)\n",
    "\n",
    "    train, dev = IMDB.splits(textField, labelField, train=\"train\", validation=\"dev\")\n",
    "\n",
    "    textField.build_vocab(train, dev, vectors=GloVe(name=\"6B\", dim=50))\n",
    "    labelField.build_vocab(train, dev)\n",
    "\n",
    "    trainLoader, testLoader = data.BucketIterator.splits((train, dev), shuffle=True, batch_size=64,\n",
    "                                                         sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "\n",
    "    net = Network().to(device)\n",
    "    \n",
    "    criterion =lossFunc()\n",
    "    optimiser = topti.Adam(net.parameters(), lr=1e-3)  # Minimise the loss using the Adam algorithm.\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimiser,step_size=2, gamma=0.7)\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0\n",
    "        for i, batch in enumerate(trainLoader):\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # PyTorch calculates gradients by accumulating contributions to them (useful for\n",
    "            # RNNs).  Hence we must manually set them to zero before calculating them.\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # Forward pass through the network.\n",
    "            output = net(inputs, length)\n",
    "\n",
    "            loss = criterion(output.view(-1), labels)\n",
    "\n",
    "            # Calculate gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Minimise the loss according to the gradient.\n",
    "            optimiser.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            if i % 32 == 31:\n",
    "                print(\"Epoch: %2d, Batch: %4d, Loss: %.3f\" % (epoch + 1, i + 1, running_loss / 32))\n",
    "                running_loss = 0\n",
    "        scheduler.step()\n",
    "    num_correct = 0\n",
    "\n",
    "    # Save mode\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate network on the test dataset.  We aren't calculating gradients, so disable autograd to speed up\n",
    "    # computations and reduce memory usage.\n",
    "    with torch.no_grad():\n",
    "        for batch in testLoader:\n",
    "            # Get a batch and potentially send it to GPU memory.\n",
    "            inputs, length, labels = textField.vocab.vectors[batch.text[0]].to(device), batch.text[1].to(\n",
    "                device), batch.label.type(torch.FloatTensor).to(device)\n",
    "\n",
    "            labels -= 1\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = torch.sigmoid(net(inputs, length))\n",
    "            predicted = torch.round(outputs.view(-1))\n",
    "            num_correct += torch.sum(labels == predicted).item()\n",
    "\n",
    "    accuracy = 100 * num_correct / len(dev)\n",
    "    print(f\"Classification accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "82.93854033290653"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
